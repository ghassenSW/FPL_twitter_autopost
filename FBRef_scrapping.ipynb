{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3hl6GxJsot7X"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import tweepy\n",
        "from datetime import datetime\n",
        "from google.colab import files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D-hW8e3tozcC"
      },
      "outputs": [],
      "source": [
        "def url_to_df(url,key=None):\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "      data = response.json()\n",
        "      if key!=None:\n",
        "        df=pd.DataFrame(data[key])\n",
        "      else:\n",
        "        df=pd.DataFrame(data)\n",
        "      return df\n",
        "  else:\n",
        "      print(f\"Error: {response.status_code}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c4t__BgN_8PJ",
        "outputId": "1fcc19c8-5f89-41c2-c954-0570e0e1fe88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ScraperFC\n",
            "  Downloading ScraperFC-3.1.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botasaurus (from ScraperFC)\n",
            "  Downloading botasaurus-4.0.75.tar.gz (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bs4 (from ScraperFC)\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting cloudscraper (from ScraperFC)\n",
            "  Downloading cloudscraper-1.2.71-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Collecting datetime (from ScraperFC)\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from ScraperFC) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ScraperFC) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ScraperFC) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ScraperFC) (2.32.3)\n",
            "Collecting selenium (from ScraperFC)\n",
            "  Downloading selenium-4.26.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ScraperFC) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from botasaurus->ScraperFC) (5.9.5)\n",
            "Collecting javascript_fixes (from botasaurus->ScraperFC)\n",
            "  Downloading javascript_fixes-1.1.29.tar.gz (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m610.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from botasaurus->ScraperFC) (1.4.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from botasaurus->ScraperFC) (4.12.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from botasaurus->ScraperFC) (3.1.5)\n",
            "Collecting close_chrome (from botasaurus->ScraperFC)\n",
            "  Downloading close_chrome-4.0.40.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botasaurus-api (from botasaurus->ScraperFC)\n",
            "  Downloading botasaurus_api-4.0.9.tar.gz (8.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botasaurus-driver (from botasaurus->ScraperFC)\n",
            "  Downloading botasaurus_driver-4.0.66.tar.gz (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.9/263.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bota (from botasaurus->ScraperFC)\n",
            "  Downloading bota-4.0.69.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botasaurus-proxy-authentication (from botasaurus->ScraperFC)\n",
            "  Downloading botasaurus_proxy_authentication-1.0.16.tar.gz (4.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botasaurus-requests (from botasaurus->ScraperFC)\n",
            "  Downloading botasaurus_requests-4.0.38.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.10/dist-packages (from cloudscraper->ScraperFC) (3.2.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from cloudscraper->ScraperFC) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ScraperFC) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ScraperFC) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ScraperFC) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ScraperFC) (2024.8.30)\n",
            "Collecting zope.interface (from datetime->ScraperFC)\n",
            "  Downloading zope.interface-7.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime->ScraperFC) (2024.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->ScraperFC) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->ScraperFC) (2024.2)\n",
            "Collecting trio~=0.17 (from selenium->ScraperFC)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium->ScraperFC)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium->ScraperFC) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium->ScraperFC) (1.8.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.2->botasaurus->ScraperFC) (2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->ScraperFC) (1.16.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->ScraperFC) (24.2.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium->ScraperFC)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.17->selenium->ScraperFC)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->ScraperFC) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium->ScraperFC) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium->ScraperFC)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium->ScraperFC) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from bota->botasaurus->ScraperFC) (8.1.7)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from botasaurus-driver->botasaurus->ScraperFC) (1.2.14)\n",
            "Collecting pyvirtualdisplay (from botasaurus-driver->botasaurus->ScraperFC)\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from botasaurus-requests->botasaurus->ScraperFC) (0.27.2)\n",
            "Collecting geventhttpclient (from botasaurus-requests->botasaurus->ScraperFC)\n",
            "  Downloading geventhttpclient-2.3.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from botasaurus-requests->botasaurus->ScraperFC) (13.9.4)\n",
            "Collecting gevent (from botasaurus-requests->botasaurus->ScraperFC)\n",
            "  Downloading gevent-24.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->botasaurus->ScraperFC) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime->ScraperFC) (75.1.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->ScraperFC) (0.14.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->botasaurus-driver->botasaurus->ScraperFC) (1.16.0)\n",
            "Collecting zope.event (from gevent->botasaurus-requests->botasaurus->ScraperFC)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: greenlet>=3.1.1 in /usr/local/lib/python3.10/dist-packages (from gevent->botasaurus-requests->botasaurus->ScraperFC) (3.1.1)\n",
            "Collecting brotli (from geventhttpclient->botasaurus-requests->botasaurus->ScraperFC)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->botasaurus-requests->botasaurus->ScraperFC) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->botasaurus-requests->botasaurus->ScraperFC) (1.0.6)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->botasaurus-requests->botasaurus->ScraperFC) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->botasaurus-requests->botasaurus->ScraperFC) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->botasaurus-requests->botasaurus->ScraperFC) (0.1.2)\n",
            "Downloading ScraperFC-3.1.2-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading cloudscraper-1.2.71-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.26.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading zope.interface-7.1.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading gevent-24.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geventhttpclient-2.3.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (112 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Building wheels for collected packages: botasaurus, bota, botasaurus-api, botasaurus-driver, botasaurus-proxy-authentication, botasaurus-requests, close_chrome, javascript_fixes\n",
            "  Building wheel for botasaurus (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for botasaurus: filename=botasaurus-4.0.75-py3-none-any.whl size=74134 sha256=f37e8cf177c77d559d8a61f2ef298dc58f694ec48b4cc3fc85b054d4df61b9db\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/98/b9/72540ea9b3759414d8eb7bdc396ce6180231f442a36fa456f0\n",
            "  Building wheel for bota (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bota: filename=bota-4.0.69-py3-none-any.whl size=17312 sha256=6b85995ed8296605957ced5e9e85d5a64c6244989aff5b80123020dd121e71f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/16/c8/23b724892f099fe179da6eedb7d173e6c93bc945202e8a3cb4\n",
            "  Building wheel for botasaurus-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for botasaurus-api: filename=botasaurus_api-4.0.9-py3-none-any.whl size=7530 sha256=c2ebc1594ce59763267d8ed029da8686dd1ced91b26b42376f8fad26701332bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/36/67/e015ba401ab36fd80c4a9f806cbbfc428897654fce7e430d7d\n",
            "  Building wheel for botasaurus-driver (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for botasaurus-driver: filename=botasaurus_driver-4.0.66-py3-none-any.whl size=301556 sha256=3b58ff39230825e80bb0f60f7d9cbcaccbc868c11cfbc3a6c2aa6739147edfd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/8c/d1/865552065b96569eb8fe3c6c8957173f366baefff2df99e2ca\n",
            "  Building wheel for botasaurus-proxy-authentication (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for botasaurus-proxy-authentication: filename=botasaurus_proxy_authentication-1.0.16-py3-none-any.whl size=4481 sha256=aa116386f529f08a7849f8c191496ecf995fbc9d51ce132f7fe351de8624cee6\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/42/8f/17b22b7200216f10b5c0c750354dd7d05eb272bf6701921188\n",
            "  Building wheel for botasaurus-requests (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for botasaurus-requests: filename=botasaurus_requests-4.0.38-py3-none-any.whl size=41995 sha256=fe3d96b0514596b298e8e1f55eab1df0658e9e6ef4a4857e1eb9d47cbd7640f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/04/7c/ab6eeef202ef9bd5d69f0e0ac82519d67af08cc6ab831c40ed\n",
            "  Building wheel for close_chrome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for close_chrome: filename=close_chrome-4.0.40-py3-none-any.whl size=3042 sha256=d3d561d41bdd180eac1676569b602388f10441a7a209c4503b077d4d4446ee96\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/d2/4d/0b00728ee8f1454fb0d9789fd12656fba6ed5dd889c1db561a\n",
            "  Building wheel for javascript_fixes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for javascript_fixes: filename=javascript_fixes-1.1.29-py3-none-any.whl size=35989 sha256=3ae0a58656220de7364cdf2e45f259b3388906acf3cacdae7ecd011d75c6df3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/07/d5/f34259ec744a10e86bb99b0edb1c93ab8d19d2bee1de22371b\n",
            "Successfully built botasaurus bota botasaurus-api botasaurus-driver botasaurus-proxy-authentication botasaurus-requests close_chrome javascript_fixes\n",
            "Installing collected packages: sortedcontainers, pyvirtualdisplay, brotli, zope.interface, zope.event, wsproto, outcome, javascript_fixes, close_chrome, bota, trio, gevent, datetime, bs4, botasaurus-proxy-authentication, botasaurus-api, trio-websocket, geventhttpclient, cloudscraper, botasaurus-driver, selenium, botasaurus-requests, botasaurus, ScraperFC\n",
            "Successfully installed ScraperFC-3.1.2 bota-4.0.69 botasaurus-4.0.75 botasaurus-api-4.0.9 botasaurus-driver-4.0.66 botasaurus-proxy-authentication-1.0.16 botasaurus-requests-4.0.38 brotli-1.1.0 bs4-0.0.2 close_chrome-4.0.40 cloudscraper-1.2.71 datetime-5.5 gevent-24.11.1 geventhttpclient-2.3.1 javascript_fixes-1.1.29 outcome-1.3.0.post0 pyvirtualdisplay-3.0 selenium-4.26.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0 zope.event-5.0 zope.interface-7.1.1\n"
          ]
        }
      ],
      "source": [
        "pip install ScraperFC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51,
          "referenced_widgets": [
            "3a72b6a77dfe4538843571239fd5ac57",
            "c8020374fea94eafaaf1dab9bbee64ac"
          ]
        },
        "id": "LhIcbaTWo2PC",
        "outputId": "787d52bf-db40-49a6-dbb6-f7cfa070fa84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading @request dependencies...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a72b6a77dfe4538843571239fd5ac57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('./src')\n",
        "\n",
        "import ScraperFC as sfc\n",
        "fb = sfc.FBref()\n",
        "sc = sfc.Sofascore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff9ZtjM_cl0b"
      },
      "source": [
        "Scrape Begins"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "LGYb7i7vNvTo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def prepare_sc(match_id):\n",
        "  matchy=sc.get_match_dict(match_id)\n",
        "  matchy={'event':matchy}\n",
        "  matchy=pd.DataFrame(matchy)\n",
        "  score=matchy.loc[['homeScore','awayScore']]\n",
        "  score.iloc[0,0]=score.iloc[0,0]['normaltime']\n",
        "  score.iloc[1,0]=score.iloc[1,0]['normaltime']\n",
        "  score.index=['home','away']\n",
        "  score.columns=['Goals']\n",
        "  teams=matchy.loc[['homeTeam','awayTeam']]\n",
        "  teams.iloc[0,0]=teams.iloc[0,0]['name']\n",
        "  teams.iloc[1,0]=teams.iloc[1,0]['name']\n",
        "  teams.index=['home','away']\n",
        "  teams.columns=['team']\n",
        "\n",
        "  stats=sc.scrape_team_match_stats(match_id)\n",
        "  stats=stats.T\n",
        "  stats.columns=stats.iloc[0]\n",
        "  stats=stats.drop(['name'],axis=0)\n",
        "  stats=stats[['Expected goals','Total shots','Shots inside box','Shots on target','Big chances']]\n",
        "  stats=stats.T\n",
        "  stats=stats[stats['period']=='ALL']\n",
        "  stats=stats.T\n",
        "  stats.columns=['xG','Shots','Total_shots','SiB','SoT','BC']\n",
        "  stats=stats.drop(['Total_shots'],axis=1)\n",
        "  stats=stats.loc[['home','away']]\n",
        "  stats=pd.concat([teams,score,stats],axis=1)\n",
        "\n",
        "  num_gw=matchy.loc['roundInfo'].iloc[0]['round']\n",
        "  num_season=matchy.loc['season'].iloc[0]['year']\n",
        "  stats.index=['H','A']\n",
        "  df=pd.DataFrame({'season':[num_season],'GW':[num_gw]})\n",
        "  df.index=df['season']\n",
        "  df=df.drop(['season'],axis=1)\n",
        "  for col in stats.columns:\n",
        "    for index,row in stats.iterrows():\n",
        "      new_col=col+' '+index\n",
        "      new_df={new_col:[row[col]]}\n",
        "      new_df=pd.DataFrame(new_df)\n",
        "      new_df.index=df.index\n",
        "      df=pd.concat([df,pd.DataFrame(new_df)],axis=1)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "SWifIViiXM9B"
      },
      "outputs": [],
      "source": [
        "def get_teams_dict(year_fb,year_sc):\n",
        "  events=sc.get_match_dicts(year_sc,'EPL')\n",
        "  all_stats=[]\n",
        "  for event in events:\n",
        "    id=event['id']\n",
        "    try:\n",
        "      stats=prepare_sc(id)\n",
        "      all_stats.append(stats)\n",
        "    except Exception as e:\n",
        "      continue\n",
        "  all_stats=pd.concat(all_stats)\n",
        "  all_stats=all_stats.sort_values(['GW'])\n",
        "  teams_sc=set()\n",
        "  for team in all_stats.loc[:,'team H']:\n",
        "    teams_sc.add(team)\n",
        "  teams_sc=sorted(teams_sc)\n",
        "  teams_fb=set()\n",
        "  for team in matches.loc[:,'Home Team']:\n",
        "    teams_fb.add(team)\n",
        "  teams_fb=sorted(teams_fb)\n",
        "  teams_fb\n",
        "  teams=dict(zip(teams_fb,teams_sc))\n",
        "  return teams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "kpvUCk6zA6Xh"
      },
      "outputs": [],
      "source": [
        "def get_xg(game):\n",
        "  home_xg=game['Home Player Stats']['Summary'].iloc[-1,18]\n",
        "  away_xg=game['Away Player Stats']['Summary'].iloc[-1,18]\n",
        "  return home_xg,away_xg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0plOsK1zvTvq"
      },
      "outputs": [],
      "source": [
        "years_fb=['2017-2018','2018-2019','2019-2020','2020-2021','2021-2022','2022-2023','2023-2024','2024-2025']\n",
        "years_sc=['17/18','18/19','19/20','20/21','21/22','22/23','23/24','24/25']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H15blKcfCIE"
      },
      "outputs": [],
      "source": [
        "excel_file='EPL_data.xlsx'\n",
        "with pd.ExcelWriter(excel_file) as writer:\n",
        "  for i,year_fb in enumerate(years_fb):\n",
        "    year_sc=years_sc[i]\n",
        "    events=sc.get_match_dicts(year_sc,'EPL')\n",
        "    all_stats=[]\n",
        "    for event in events:\n",
        "      id=event['id']\n",
        "      try:\n",
        "        stats=prepare_sc(id)\n",
        "        all_stats.append(stats)\n",
        "      except Exception as e:\n",
        "        continue\n",
        "    all_stats=pd.concat(all_stats)\n",
        "    all_stats=all_stats.sort_values(['GW'])\n",
        "\n",
        "    all_stats.insert(5,'xG H',np.nan)\n",
        "    all_stats.insert(6,'xG A',np.nan)\n",
        "    all_stats.index=range(len(all_stats))\n",
        "    matches=fb.scrape_matches(year_fb,'EPL')\n",
        "    teams_fb_sc=get_teams_dict(year_fb,year_sc)\n",
        "    for i,matchy in matches.iterrows():\n",
        "      xg_h,xg_a=get_xg(matchy)\n",
        "      team_h=teams_fb_sc[matchy.loc['Home Team']]\n",
        "      team_a=teams_fb_sc[matchy.loc['Away Team']]\n",
        "      if all_stats[(all_stats['team H']==team_h) & (all_stats['team A']==team_a)].empty:\n",
        "        continue\n",
        "      ind=all_stats[(all_stats['team H']==team_h) & (all_stats['team A']==team_a)].index[0]\n",
        "      all_stats.iloc[ind,5]=xg_h\n",
        "      all_stats.iloc[ind,6]=xg_a\n",
        "\n",
        "    all_stats=all_stats.astype({'GW':'int64','Goals H':'int64','Goals A':'int64','xG H':'float','xG A':'float','Shots H':'int64','Shots A':'int64','SiB H':'int64','SiB A':'int64','SoT H':'int64','SoT A':'int64','BC H':'int64','BC A':'int64'})\n",
        "    all_stats.to_excel(writer, sheet_name=year_fb, index=False)\n",
        "files.download(excel_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "0_4DuA6Rxsn6",
        "outputId": "e1a0b212-f7b8-4948-b308-ca38c5387dce"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_9defdf95-bcac-4c22-bb72-a6de64231f1f\", \"all_stats.csv\", 6918)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "all_stats.to_csv('all_stats.csv')\n",
        "file_path = 'all_stats.csv'\n",
        "files.download(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1RA4ZY7vva4"
      },
      "outputs": [],
      "source": [
        "excel_file='EPL_data.xlsx'\n",
        "with pd.ExcelWriter(excel_file) as writer:\n",
        "  for year in years_fb:\n",
        "    all_stats.to_excel(writer, sheet_name=year, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a72b6a77dfe4538843571239fd5ac57": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_c8020374fea94eafaaf1dab9bbee64ac",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[35m 51%\u001b[0m \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/13.8 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080\"> 51%</span> <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━</span><span style=\"color: #3a3a3a; text-decoration-color: #3a3a3a\">╺━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #008000; text-decoration-color: #008000\">7.0/13.8 MB</span> <span style=\"color: #800000; text-decoration-color: #800000\">33.7 MB/s</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "c8020374fea94eafaaf1dab9bbee64ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}